{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNNnmi9lVyiXimNoy4IY/ud",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruv20047228/LLM/blob/main/GPT_model_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GPT model from scratch**"
      ],
      "metadata": {
        "id": "TIPDjZA-qsmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvlrSwH8fbgZ"
      },
      "outputs": [],
      "source": [
        "from importlib.metadata import version\n",
        "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
        "print(\"torch version:\",version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257, #vocabulary size\n",
        "    \"context_length\": 1024, #maximum input token count\n",
        "    \"emb_dim\": 768, #size of input tokens\n",
        "    \"n_heads\": 12, # number of attention heads\n",
        "    \"n_layers\": 12, #transformer block\n",
        "    \"drop_rate\": 0.1, #dropout mechanism intensity\n",
        "    \"qkv_bias\": False  #decides to use bias vector in QKV computation or not\n",
        "}"
      ],
      "metadata": {
        "id": "mKEJD5A56sZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "cWZStqiE7AkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyGPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[DummyTransformerBlock(cfg[\"emb_dim\"])\n",
        "        for _ in range(cfg[\"n_layers\"])]\n",
        "    )\n",
        "    self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(\n",
        "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "    )\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds # Assign value to x\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "  def __init__(self, normalized_shape, eps=1e-5):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "gvVkuLja8BPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "batch = []\n",
        "\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "id": "N2a3nvj_9JSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "logits = model(batch)\n",
        "print(\"Output shape:\", logits.shape)\n",
        "print(logits)"
      ],
      "metadata": {
        "id": "NSDyKe_rU1zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Layer normalisation\n",
        "torch.manual_seed(123)\n",
        "\n",
        "batch_example = torch.randn(2, 5)\n",
        "\n",
        "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "out = layer(batch_example)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "S5kuzsyAWZxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#computation of mean and variance\n",
        "mean  =  out.mean(dim=-1, keepdim=True) #dim=1 means mean per row & dim=0 means mean per column\n",
        "var = out.var(dim=-1, keepdim=True)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", mean)"
      ],
      "metadata": {
        "id": "CV8pqLLzXHw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_norm = (out - mean) / torch.sqrt(var)\n",
        "print(\"Normalized ;ayer outputs:\\n\", out_norm)\n",
        "\n",
        "mean = out.mean(dim=-1, keepdim=True)\n",
        "var = out_norm.var(dim=-1, keepdim=True)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Var:\\n\", var)"
      ],
      "metadata": {
        "id": "NvXhiPqgX1nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(sci_mode=False)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Var:\\n\", var) #till now we normalized each input"
      ],
      "metadata": {
        "id": "TXcjnOltYQGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation of LayerNorm\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=True)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return  self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "id": "IP4XrjrmYd58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ln = LayerNorm(emb_dim=5)\n",
        "out_ln = ln(batch_example)"
      ],
      "metadata": {
        "id": "ZO4B8jY6aYZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = out_ln.mean(dim=-1, keepdim=True)\n",
        "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
        "\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "id": "2cPk1BDLag03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implementing a feed forward network with GELU activations**"
      ],
      "metadata": {
        "id": "x4AUi5sua8zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * ( 1 + torch.tanh(\n",
        "        torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "        (x + 0.044715 * torch.pow(x, 3))\n",
        "    ))"
      ],
      "metadata": {
        "id": "bLmI0uNubCar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "gelu, relu = GELU(), nn.ReLU()\n",
        "\n",
        "x = torch.linspace(-3, 3, 100)\n",
        "y_gelu, y_relu = gelu(x), relu(x)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
        "  plt.subplot(1, 2, i)\n",
        "  plt.plot(x, y)\n",
        "  plt.title(f\"{label} activation function\")\n",
        "  plt.xlabel(\"x\")\n",
        "  plt.ylabel(f\"{label}(x)\")\n",
        "  plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tutSNzPDcDrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#small neural network module\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "GQsBYCObdPWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(GPT_CONFIG_124M[\"emb_dim\"])"
      ],
      "metadata": {
        "id": "F2H2kZMcd2nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffn = FeedForward(GPT_CONFIG_124M)\n",
        "\n",
        "x = torch.rand(2, 3, 768)\n",
        "out = ffn(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "lj4RwXTje1BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Adding shortcut connection**"
      ],
      "metadata": {
        "id": "LtJPEPmxfVeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#shortcut connection helps in maintaining larger gradient value on the early layers of neural network also\n",
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "  def __init__(self, layer_sizes, use_shortcut):\n",
        "    super().__init__()\n",
        "    self.use_shortcut = use_shortcut\n",
        "    self.layers = nn.ModuleList([\n",
        "        nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]),GELU()),\n",
        "        nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]),GELU()),\n",
        "        nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]),GELU()),\n",
        "        nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]),GELU()),\n",
        "        nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]),GELU())\n",
        "    ])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      layer_output = layer(x)\n",
        "      if self.use_shortcut and x.shape == layer_output.shape:\n",
        "        x = x + layer_output\n",
        "      else :\n",
        "        x = layer_output\n",
        "    return x\n",
        "\n",
        "def print_gradients(model, x):\n",
        "    output = model(x)\n",
        "    target = torch.tensor([[0.]])\n",
        "\n",
        "    loss = nn.MSELoss()\n",
        "    loss = loss(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "      if 'weight' in name:\n",
        "        print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
      ],
      "metadata": {
        "id": "8rVZyC1dfRGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes = [3,3,3,3,3,1]\n",
        "\n",
        "sample_input = torch.tensor([[1., 0., -1.]])\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
        "    layer_sizes, use_shortcut=False\n",
        ")\n",
        "print_gradients(model_without_shortcut, sample_input)"
      ],
      "metadata": {
        "id": "wcp2N44GiC0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#printing values with shortcut connection\n"
      ],
      "metadata": {
        "id": "MI7nmWKli0N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
        "    layer_sizes, use_shortcut=True\n",
        ")\n",
        "print_gradients(model_with_shortcut, sample_input)"
      ],
      "metadata": {
        "id": "SvI3Ol8fi3ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Connection of attention and linear layer into transformer block**"
      ],
      "metadata": {
        "id": "HwXb3G9FjXr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from previous_chapters import MultiHeadAttention\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(\n",
        "        d_in=cfg[\"emb_dim\"],\n",
        "        d_out=cfg[\"emb_dim\"],\n",
        "        context_length=cfg[\"context_length\"],\n",
        "        num_heads=cfg[\"n_heads\"],\n",
        "        dropout=cfg[\"drop_rate\"],\n",
        "        qkv_bias=cfg[\"qkv_bias\"]\n",
        "    )\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "qckcSDEfjhL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "x = torch.rand(2, 4, 768)\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "output = block(x)\n",
        "\n",
        "print(\"Input shape: \", x.shape)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "id": "btGZCShPlnJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Coding the GPT Model**\n"
      ],
      "metadata": {
        "id": "gd4a9PTgmU7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transformer block is repeated for 12 times\n",
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "    )\n",
        "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(\n",
        "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "    )\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "Jnb1wnK2mXrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate GPT Model\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "out = model(batch)\n",
        "print(\"Input Output:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "8dsiSZJwoqOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "xeCqXyMRpJXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ],
      "metadata": {
        "id": "qFU37GUWpnyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#subtraction of number of parameters of output layer leads to 124M parameter model\n",
        "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
        "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
      ],
      "metadata": {
        "id": "um1mN3Acp6HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_size_bytes = total_params * 4\n",
        "total_size_mb = total_size_bytes / (1024 * 1024)\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "id": "_0XpuunjqnV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Generating text**"
      ],
      "metadata": {
        "id": "yEwQGezBq50a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "\n",
        "    logits = logits[:, -1, :]\n",
        "\n",
        "    probas = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "  return idx"
      ],
      "metadata": {
        "id": "Zf0wY_BPq8nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction and Appending of words\n",
        "start_context = \"Hello, I am\"\n",
        "\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(\"encoded:\", encoded)\n",
        "\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
      ],
      "metadata": {
        "id": "IqltJZLQsWM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "out = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=encoded_tensor,\n",
        "    max_new_tokens=6,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output:\", out)\n",
        "print(\"Output length:\", len(out[0]))"
      ],
      "metadata": {
        "id": "9cgIU8lus1SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove batch dimension and convert it back into text\n",
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "id": "yj3GHFf2to6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}